"""
ì™¸ë¶€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
scitimeê³¼ OpenMLë¡œë¶€í„° ì‹¤ì œ ë¶„ì„ ì‹œê°„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import numpy as np
from datetime import datetime
import os


class BenchmarkDataFetcher:
    """ì™¸ë¶€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.benchmarks = []
        
    def fetch_openml_data(self, limit=1000):
        """OpenMLì—ì„œ ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        print("=" * 60)
        print("OpenML ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        print("=" * 60)
        
        try:
            import openml
            
            # OpenML ì‹¤í–‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            print(f"\nğŸ“¥ ìµœê·¼ {limit}ê°œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
            
            # scikit-learn ê´€ë ¨ ì‹¤í–‰ ê²°ê³¼ë§Œ í•„í„°ë§
            runs_df = openml.runs.list_runs(
                size=limit,
                output_format='dataframe'
            )
            
            if runs_df is None or runs_df.empty:
                print("âš ï¸  OpenML ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            print(f"âœ… {len(runs_df)}ê°œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            
            # ë°ì´í„° ë³€í™˜
            collected = 0
            for idx, run in runs_df.iterrows():
                try:
                    # í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
                    if pd.isna(run.get('run_time')):
                        continue
                    
                    # ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    dataset_id = run.get('data_id')
                    if pd.isna(dataset_id):
                        continue
                    
                    try:
                        dataset = openml.datasets.get_dataset(int(dataset_id))
                        rows, cols, _, _ = dataset.get_data()
                        n_rows = len(rows) if rows is not None else 0
                        n_cols = len(cols) if cols is not None else 0
                    except:
                        continue
                    
                    if n_rows == 0 or n_cols == 0:
                        continue
                    
                    # ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ ë§¤í•‘
                    flow_name = str(run.get('flow_name', ''))
                    method = self._map_algorithm_name(flow_name)
                    
                    if method == 'unknown':
                        continue
                    
                    benchmark = {
                        'source': 'openml',
                        'timestamp': datetime.now().isoformat(),
                        'rows': int(n_rows),
                        'columns': int(n_cols),
                        'method': method,
                        'tool': 'python',
                        'hardware': 'medium',  # OpenML ì„œë²„ëŠ” ëŒ€ëµ ì¤‘ê°„ ì‚¬ì–‘
                        'data_type_ratio': {
                            'numeric': 0.7,
                            'categorical': 0.2,
                            'text': 0.1
                        },
                        'elapsed_time_seconds': float(run['run_time']),
                        'loading_time': float(run['run_time']) * 0.2,
                        'preprocessing_time': float(run['run_time']) * 0.3,
                        'execution_time': float(run['run_time']) * 0.5
                    }
                    
                    self.benchmarks.append(benchmark)
                    collected += 1
                    
                    if collected % 10 == 0:
                        print(f"  ìˆ˜ì§‘ ì¤‘... {collected}ê°œ")
                    
                    if collected >= 100:  # ë„ˆë¬´ ë§ì´ ìˆ˜ì§‘í•˜ì§€ ì•Šë„ë¡ ì œí•œ
                        break
                        
                except Exception as e:
                    continue
            
            print(f"\nâœ… OpenMLì—ì„œ {collected}ê°œì˜ ìœ íš¨í•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return self.benchmarks
            
        except ImportError:
            print("âŒ openml íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜: pip install openml")
            return []
        except Exception as e:
            print(f"âŒ OpenML ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _map_algorithm_name(self, flow_name):
        """OpenMLì˜ ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ì„ ìš°ë¦¬ ì‹œìŠ¤í…œì˜ ë°©ë²•ëª…ìœ¼ë¡œ ë§¤í•‘"""
        flow_name = flow_name.lower()
        
        # ë¶„ë¥˜
        if 'randomforest' in flow_name or 'random_forest' in flow_name:
            return 'clf_forest'
        elif 'logistic' in flow_name:
            return 'clf_logistic'
        elif 'decisiontree' in flow_name or 'decision_tree' in flow_name:
            return 'clf_tree'
        elif 'svm' in flow_name or 'svc' in flow_name:
            return 'clf_svm'
        
        # íšŒê·€
        elif 'linearregression' in flow_name or 'linear_regression' in flow_name:
            return 'reg_linear_multiple'
        elif 'ridge' in flow_name:
            return 'reg_ridge'
        
        # í´ëŸ¬ìŠ¤í„°ë§
        elif 'kmeans' in flow_name:
            return 'clu_kmeans_small'
        elif 'dbscan' in flow_name:
            return 'clu_dbscan'
        
        return 'unknown'
    
    def generate_scitime_inspired_data(self):
        """scitime ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ì°¸ê³ í•œ í•©ì„± ë°ì´í„° ìƒì„±
        
        scitimeì€ ì‹¤ì œë¡œ ì‘ì€ ìƒ˜í”Œì„ ì¸¡ì •í•˜ê³  ì™¸ì‚½í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
        ì—¬ê¸°ì„œëŠ” scitimeì˜ ë³µì¡ë„ ê³µì‹ì„ ì°¸ê³ í•˜ì—¬ í˜„ì‹¤ì ì¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print("\n" + "=" * 60)
        print("scitime ë°©ë²•ë¡  ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìƒì„±...")
        print("=" * 60)
        
        # scitimeì´ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ë³„ ë³µì¡ë„ (ë…¼ë¬¸ ì°¸ê³ )
        algorithms = [
            # ì§‘ê³„ - O(n)
            {'method': 'agg_basic', 'complexity': lambda n, d: n * d * 1e-7},
            {'method': 'agg_groupby', 'complexity': lambda n, d: n * np.log(n) * d * 1e-7},
            
            # íšŒê·€ - O(n * d^2)
            {'method': 'reg_linear_simple', 'complexity': lambda n, d: n * d * 1e-6},
            {'method': 'reg_linear_multiple', 'complexity': lambda n, d: n * d * d * 1e-6},
            {'method': 'reg_ridge', 'complexity': lambda n, d: n * d * d * 1.2e-6},
            
            # ë¶„ë¥˜ - ë³µì¡ë„ ë†’ìŒ
            {'method': 'clf_logistic', 'complexity': lambda n, d: n * d * 100 * 1e-6},  # 100 iterations
            {'method': 'clf_tree', 'complexity': lambda n, d: n * np.log(n) * d * 2e-6},
            {'method': 'clf_forest', 'complexity': lambda n, d: n * np.log(n) * d * 100 * 2e-6},  # 100 trees
            
            # í´ëŸ¬ìŠ¤í„°ë§
            {'method': 'clu_kmeans_small', 'complexity': lambda n, d: n * 5 * d * 100 * 1e-6},  # k=5, iter=100
            {'method': 'clu_kmeans_large', 'complexity': lambda n, d: n * 15 * d * 100 * 1e-6},  # k=15
        ]
        
        # ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°
        data_sizes = [
            {'rows': 1000, 'columns': 5},
            {'rows': 5000, 'columns': 10},
            {'rows': 10000, 'columns': 10},
            {'rows': 50000, 'columns': 20},
            {'rows': 100000, 'columns': 20},
            {'rows': 500000, 'columns': 30},
            {'rows': 1000000, 'columns': 30},
            {'rows': 1000000, 'columns': 50},
        ]
        
        # í•˜ë“œì›¨ì–´ ë°°ìœ¨
        hardware_configs = [
            {'name': 'low', 'multiplier': 4.0},
            {'name': 'medium', 'multiplier': 1.0},
            {'name': 'high', 'multiplier': 0.4},
            {'name': 'ultra', 'multiplier': 0.2},
        ]
        
        generated = 0
        for algo in algorithms:
            for size in data_sizes:
                for hw in hardware_configs:
                    # ê¸°ë³¸ ì‹œê°„ ê³„ì‚°
                    base_time = algo['complexity'](size['rows'], size['columns'])
                    
                    # í•˜ë“œì›¨ì–´ ë°°ìœ¨ ì ìš©
                    adjusted_time = base_time * hw['multiplier']
                    
                    # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì ìœ¼ë¡œ)
                    noise = np.random.normal(1.0, 0.1)
                    final_time = adjusted_time * noise
                    
                    # ìµœì†Œ ì‹œê°„ ë³´ì¥
                    final_time = max(final_time, 0.001)
                    
                    benchmark = {
                        'source': 'scitime_inspired',
                        'timestamp': datetime.now().isoformat(),
                        'rows': size['rows'],
                        'columns': size['columns'],
                        'method': algo['method'],
                        'tool': 'python',
                        'hardware': hw['name'],
                        'data_type_ratio': {
                            'numeric': 0.7,
                            'categorical': 0.2,
                            'text': 0.1
                        },
                        'elapsed_time_seconds': round(final_time, 4),
                        'loading_time': round(final_time * 0.2, 4),
                        'preprocessing_time': round(final_time * 0.3, 4),
                        'execution_time': round(final_time * 0.5, 4)
                    }
                    
                    self.benchmarks.append(benchmark)
                    generated += 1
        
        print(f"âœ… {generated}ê°œì˜ scitime ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        return self.benchmarks
    
    def save_benchmarks(self, filename='benchmark_data.json'):
        """ìˆ˜ì§‘ëœ ë²¤ì¹˜ë§ˆí¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not self.benchmarks:
            print("âš ï¸  ì €ì¥í•  ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.benchmarks, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ {len(self.benchmarks)}ê°œì˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ '{filename}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    def get_summary(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½"""
        if not self.benchmarks:
            return "ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ"
        
        df = pd.DataFrame(self.benchmarks)
        
        summary = f"""
{'=' * 60}
ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìš”ì•½
{'=' * 60}
ì´ ê°œìˆ˜: {len(self.benchmarks):,}ê°œ

ì¶œì²˜ë³„:
{df['source'].value_counts().to_string() if 'source' in df.columns else 'N/A'}

ë°ì´í„° í¬ê¸° ë²”ìœ„:
  - í–‰: {df['rows'].min():,} ~ {df['rows'].max():,}
  - ì—´: {df['columns'].min()} ~ {df['columns'].max()}

ë¶„ì„ ë°©ë²•:
{df['method'].value_counts().to_string()}

í•˜ë“œì›¨ì–´:
{df['hardware'].value_counts().to_string()}

ì‹¤í–‰ ì‹œê°„ í†µê³„:
  - í‰ê· : {df['elapsed_time_seconds'].mean():.2f}ì´ˆ
  - ì¤‘ì•™ê°’: {df['elapsed_time_seconds'].median():.2f}ì´ˆ
  - ìµœì†Œ: {df['elapsed_time_seconds'].min():.4f}ì´ˆ
  - ìµœëŒ€: {df['elapsed_time_seconds'].max():.2f}ì´ˆ
{'=' * 60}
"""
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    fetcher = BenchmarkDataFetcher()
    
    print("\nğŸš€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")
    
    # 1. scitime ë°©ë²•ë¡  ê¸°ë°˜ ë°ì´í„° ìƒì„± (í•­ìƒ ê°€ëŠ¥)
    fetcher.generate_scitime_inspired_data()
    
    # 2. OpenML ë°ì´í„° ìˆ˜ì§‘ ì‹œë„ (ì„ íƒì )
    try:
        print("\nâ³ OpenML ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        print("   (ì‹¤íŒ¨í•´ë„ scitime ë°ì´í„°ë§Œìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤)")
        fetcher.fetch_openml_data(limit=200)
    except Exception as e:
        print(f"âš ï¸  OpenML ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        print("   scitime ê¸°ë°˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # 3. ê²°ê³¼ ì €ì¥
    fetcher.save_benchmarks('benchmark_data.json')
    
    # 4. ìš”ì•½ ì¶œë ¥
    print(fetcher.get_summary())
    
    print("\nâœ… ì™„ë£Œ! ì´ì œ ì›¹ ì„œë¹„ìŠ¤ì—ì„œ ì´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
