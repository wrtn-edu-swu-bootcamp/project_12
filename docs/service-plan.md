# 데이터 분석 시간 예측 서비스 기획안

## 1. 서비스 개요

### 서비스명
**Data Analysis Time Predictor**

### 핵심 가치
**"쉽고 빠르게, 분석 시간을 미리 알자!"**

### 서비스 목적
데이터 분석 시작 전에 **30초 입력 → 1초 예측**으로 분석 소요 시간을 즉시 확인

---

## 1-1. 문제 정의 (Problem Statement)

### 🎯 핵심 문제

**"데이터 분석 시간을 예측할 수 없어서 발생하는 실질적인 손실"**

### 실제 현장의 고민들

#### 1️⃣ 팀 프로젝트 상황

**문제:**
- 카페나 스터디룸에서 팀플 중 시간 제약
- "이거 1시간 안에 끝날까?" 예측 불가
- 시간 부족하면 중단하고 처음부터 다시 시작

**비용:**
- 시간 낭비: 분석 재시작으로 2배 시간 소요
- 장소 비용: 예상보다 오래 걸려 추가 비용 발생
- 팀 스트레스: 불확실성으로 인한 불안감

**실제 사례:**
```
오후 2시: "이거 3시간이면 되겠지?"
오후 4시: 아직도 50% 진행 중...
오후 5시: 장소 예약 시간 끝남
→ 분석 중단, 처음부터 다시 시작
```

#### 2️⃣ 피시방/노트북 사용 상황

**문제:**
- 피시방 시간 결제 불확실 (2시간? 4시간?)
- 자리를 떠날 수 없어 무기한 대기
- 시간 추가 결제로 비용 증가

**비용:**
- 금전적 비용: 불필요한 시간 결제
- 기회 비용: 피시방에 묶여 있는 시간
- 정신적 스트레스: 언제 끝날지 모르는 불안감

**실제 사례:**
```
2시간 결제 → 2시간 30분 소요 예상
→ 4시간 결제로 변경 (2배 비용)
→ 실제로는 2시간 10분 소요 (1시간 50분 낭비)
```

#### 3️⃣ 야간 분석 상황

**문제:**
- 밤에 돌려놓고 자는데 오류 발생
- 몇 시간 기다렸는데 허무하게 실패
- 아침에 확인하면 이미 늦음

**비용:**
- 시간 낭비: 8시간 대기 후 실패
- 일정 지연: 다음날 일정 차질
- 데드라인 압박: 제출 기한 촉박

**실제 사례:**
```
밤 11시: 분석 시작 (아침에 확인 예정)
아침 7시: 확인 → 새벽 2시에 오류로 중단됨
→ 5시간 허비, 하루 일정 지연
```

#### 4️⃣ 시간 계획 수립 상황

**문제:**
- "언제쯤 끝날까?" 알 수 없음
- 다른 일정 계획 불가능
- 효율적인 시간 배분 어려움

**비용:**
- 생산성 저하: 대기 시간 활용 못함
- 일정 충돌: 예상 못한 지연으로 약속 변경
- 의사결정 지연: 시간 모르니 최적 방법 선택 못함

**실제 사례:**
```
"점심 먹고 오면 끝날까?"
→ 모르니까 자리 지킴
→ 실제로는 10분만에 끝남
→ 점심시간 40분 낭비
```

### 📊 문제의 규모

| 상황 | 발생 빈도 | 평균 손실 시간 | 연간 누적 손실 |
|------|----------|--------------|--------------|
| 팀플 중 재시작 | 월 2-3회 | 2시간/회 | 48-72시간 |
| 피시방 대기 | 월 4-5회 | 1시간/회 | 48-60시간 |
| 야간 분석 실패 | 월 1-2회 | 5시간/회 | 60-120시간 |
| 불필요한 대기 | 주 2-3회 | 30분/회 | 52-78시간 |
| **총계** | - | - | **208-330시간/년** |

→ **연간 약 250시간 (10일) 낭비**

### 💡 해결 방향

**"실행 전에 미리 알면 모든 문제가 해결됩니다"**

1. **정확한 시간 계획**
   - 피시방 적정 시간 결제
   - 팀플 장소/시간 예약
   - 야간 분석 여부 판단

2. **효율적인 자원 활용**
   - 대기 시간에 다른 작업
   - 최적 분석 방법 선택
   - 하드웨어 업그레이드 여부 결정

3. **스트레스 감소**
   - 불확실성 제거
   - 계획적인 일정 관리
   - 팀 협업 효율 증가

---

## 1-2. 차별점

### 기존 방식 vs 본 서비스

| 구분 | 기존 방식 | 본 서비스 |
|------|---------|----------|
| 시점 | 분석 실행 후 | 분석 실행 전 ⭐ |
| 소요시간 | 전체 분석 시간 | 1초 이내 |
| 정확도 | 100% (실행 후) | ±20-25% (사전 예측) |
| 비용 | 리소스 소비 | 거의 없음 |
| 계획성 | 불가능 | 가능 ⭐ |
| 스트레스 | 높음 (불확실성) | 낮음 (예측 가능) |

### 핵심 가치 제안

**"70-80% 정확도로도 충분히 가치가 있습니다"**

```
정확도 100% (실행 후) vs 정확도 75% (실행 전)

실행 후: 2시간 소요 (정확히 알지만 이미 늦음)
실행 전: 1.5~2.5시간 예측 (±25% 오차, 계획 가능!)
→ 피시방 3시간 결제, 팀플 시간 조정 등 가능
```

---

## 2. 핵심 기능

### 입력 항목 (30초 이내)
1. **데이터 정보** 
   - 행 수, 열 수
   
2. **분석 방법** 
   - 17개 방법: 단순 집계(3) / 회귀(4) / 분류(4) / 클러스터링(4) / 딥러닝(2)
   
3. **툴 선택** 
   - Python, R, SQL, Excel
   
4. **하드웨어** 
   - 저사양 / 중간 / 고사양 / 최고사양
   - 성능 차이: 최대 5-10배
   - **사양 기준**:
     - 저사양: 2코어, 4GB RAM, HDD
     - 중간: 4코어, 8-16GB RAM, SSD
     - 고사양: 8코어, 32GB RAM, SSD
     - 최고사양: 16코어 이상, 64GB+ RAM, SSD + GPU

5. **데이터 타입** 
   - 수치형 / 범주형 / 텍스트 비율
   - 각 타입의 비율 합계 100%

### 출력 정보 (1초 이내)
1. **예상 소요 시간** ⭐
   - 중앙값 (가장 가능성 높은 시간)
   
2. **신뢰 구간** ⭐
   - 최소~최대 범위 (항상 표시)
   - 예: 30분 ~ 60분 (70% 확률)
   
3. **예측 신뢰도**
   - High (±20%) / Medium (±30%) / Low (±50%)
   
4. **단계별 분해**
   - 로딩(20%) / 전처리(30%) / 분석(50%)
   
5. **최적화 추천** (선택사항)
   - 기본: 숨김 처리
   - 토글로 활성화 가능

---

## 3. 타겟 사용자

### 주요 페르소나 (실제 경험 기반)

#### 👨‍🎓 **1. 데이터사이언스 학과 학생 (준호, 23세)**

**상황:**
- 팀 프로젝트로 데이터 분석 진행
- 카페나 스터디룸에서 작업
- 피시방에서 개인 과제 수행

**Pain Points:**
- ❌ "카페에서 2시간 예약했는데 분석이 3시간 걸리면?"
- ❌ "피시방에 무기한으로 있어야 할 때..."
- ❌ "밤에 돌려놓고 자는데 오류 나면 허무함"
- ❌ "시간 더 걸릴 것 같아서 중간에 끊고 처음부터..."

**시나리오 1: 팀플 상황**
```
오후 3시: 카페 스터디룸 5시까지 예약
문제: "랜덤 포레스트 2시간 안에 끝날까?"

서비스 사용:
→ 예측: 1시간 30분 (1시간 ~ 2시간)
→ 결정: 지금 바로 실행 ✅
→ 결과: 1시간 40분 소요, 여유있게 완료
```

**시나리오 2: 피시방 상황**
```
피시방 방문: "몇 시간 결제해야 할까?"
문제: 시간 모르니까 4시간 결제 (8,000원)

서비스 사용:
→ 예측: 2시간 (1.5 ~ 2.5시간)
→ 결정: 3시간 결제 (6,000원)
→ 결과: 2시간 10분 소요, 2,000원 절약
```

#### 👩‍💼 **2. 데이터 분석가 (민수, 28세)**

**상황:**
- 회사에서 정기 리포트 작성
- 시간 계획 필요

**Pain Points:**
- ❌ "점심 먹고 오면 끝날까?"
- ❌ "퇴근 전에 결과 나올까?"

**시나리오:**
```
오전 11시: 분석 시작 전
예측: 45분 (30 ~ 60분)
→ 점심 먹고 오기로 결정 ✅
```

#### 👨‍🔬 **3. 연구자 (수진, 26세)**

**상황:**
- 논문 작성 중 실험 필요
- 학교 서버 vs 개인 노트북 선택

**Pain Points:**
- ❌ "밤에 돌릴지 낮에 돌릴지?"
- ❌ "서버 예약해야 하는데..."

**시나리오:**
```
야간 분석 결정:
예측: 8시간 (6 ~ 10시간)
→ 오류 위험 있으니 낮에 실행 결정
→ 서버 예약 (내일 오전 9시-5시)
```

#### 👔 **4. PM/기획자 (현우, 35세)**

**상황:**
- 여러 분석 방법 중 선택
- 시간 vs 정확도 트레이드오프

**Pain Points:**
- ❌ "방법 A vs B, 뭐가 더 빠를까?"

**시나리오:**
```
3가지 방법 비교:
방법 A: 10분 예측
방법 B: 60분 예측
방법 C: 120분 예측
→ 방법 A 선택 ✅
```

### 타겟 사용자 정리

| 페르소나 | 주요 니즈 | 사용 빈도 | 해결 가치 |
|----------|----------|----------|----------|
| 학생 (팀플) | 시간 계획 | 주 2-3회 | ⭐⭐⭐⭐⭐ |
| 학생 (피시방) | 비용 절감 | 주 1-2회 | ⭐⭐⭐⭐ |
| 직장인 | 일정 관리 | 일 1-2회 | ⭐⭐⭐⭐ |
| 연구자 | 자원 배분 | 주 1회 | ⭐⭐⭐ |

---

## 4. 예측 방법론

### MVP: 벤치마크 기반
- 실제 측정 데이터에서 유사 케이스 찾기
- 하드웨어 차이 보정
- 신뢰 구간 자동 계산

### Phase 2: 메타 학습 추가
- scitime 방식 참고 (랜덤 포레스트/신경망)
- 정확도 향상: ±30% → ±20%

### 정확도 목표
| Phase | 목표 | 방법 | 상태 |
|-------|------|------|------|
| MVP | ±30% | 벤치마크 | ✅ 완료 |
| Phase 2 | ±20-25% | 벤치마크 + ML 앙상블 | ✅ 완료 |
| Phase 3 | ±15% | 데이터 확장 + 고도화 | 📋 계획 중 |

### 현재 구현 상태 (Phase 2 완료)

**구현된 기술:**
- ✅ scitime 방식 랜덤 포레스트 (200 trees)
- ✅ 3가지 예측 앙상블 (벤치마크 + ML + 복잡도)
- ✅ 동적 가중치 조정
- ✅ R² Score: 0.977

**달성한 성과:**
- ✅ 정확도: ±30% → ±20-25%
- ✅ 알고리즘 커버리지: 59% → 100%
- ✅ 신뢰 구간 폭: 39% 축소

---

## 4-1. 정확도 개선 세부 전략 (Phase 3)

### 단계 1: 데이터 확장 (예상 소요: 1-2시간)

**1.1 벤치마크 데이터 증가**
```
현재: 320개
목표: 1000개+

방법:
- 데이터 크기 조합: 8개 → 30개
- 더 세밀한 크기: (1K, 2.5K, 5K, 7.5K, 10K, 25K, 50K, 75K, 100K...)
- 실제 환경 측정 데이터 추가
```

**예상 효과:**
- 유사 케이스 발견률 3배 증가
- 신뢰 구간 20% 축소

**1.2 누락 알고리즘 데이터 보강**
```
추가 필요: SVM, 다항 회귀, DBSCAN, 계층적 클러스터링, 딥러닝
현재: scitime 기반 합성 데이터만
목표: 실제 측정 데이터 추가
```

### 단계 2: 예측 알고리즘 고도화 (예상 소요: 2-3시간)

**2.1 이상치 제거**
```python
# IQR 방식 적용
Q1 = np.percentile(times, 25)
Q3 = np.percentile(times, 75)
IQR = Q3 - Q1
필터링: Q1 - 1.5*IQR ~ Q3 + 1.5*IQR
```

**예상 효과:**
- 이상치로 인한 오차 30% 감소
- 신뢰 구간 더 정확

**2.2 하드웨어 간 스케일링**
```
아이디어: 다른 하드웨어의 데이터를 변환하여 활용

예시:
- 중간 사양에서 10분 소요
- 저사양 → 중간: 2.5배 느림
- → 저사양 예측: 25분으로 변환

효과: 유사 케이스 4배 증가 (4개 하드웨어)
```

**2.3 데이터 타입 비율 반영**
```
현재: 유사도 계산에 행/열만 고려
개선: 데이터 타입 비율도 고려

예시:
- 수치형 70% vs 수치형 90%
- 타입 차이가 크면 유사도 감소
```

**예상 효과:**
- 특수 데이터 예측 정확도 20% 향상

### 단계 3: ML 모델 개선 (예상 소요: 3-4시간)

**3.1 하이퍼파라미터 튜닝**
```python
현재:
RandomForestRegressor(n_estimators=200, max_depth=15)

개선:
GridSearchCV로 최적화
- n_estimators: [200, 300, 500]
- max_depth: [15, 20, 25]
- min_samples_split: [3, 5, 7]
```

**3.2 추가 특성 엔지니어링**
```python
현재 9개 특성:
- log_rows, log_cols, method, hardware...

추가 특성:
- log_rows * log_cols * method (3차 상호작용)
- data_type_complexity (타입 복잡도 점수)
- hardware_efficiency (하드웨어별 효율)
```

**예상 효과:**
- ML 모델 단독 정확도 5-10% 향상
- 앙상블 전체 3-5% 향상

**3.3 교차 검증 및 평가**
```python
# 5-fold 교차 검증
scores = cross_val_score(model, X, y, cv=5, 
                         scoring='neg_mean_absolute_percentage_error')

# 실제 vs 예측 비교 차트 생성
```



---

## 4. 성능 영향 요소

### 주요 요소
| 요소 | 영향도 | 시간 변화 | 
|------|--------|----------|
| 데이터 크기(행) | ⭐⭐⭐⭐⭐ | 선형 비례 |
| 하드웨어 | ⭐⭐⭐⭐⭐ | 2-10배 |
| 툴/라이브러리 | ⭐⭐⭐⭐ | 1-25배 |
| 데이터 타입 | ⭐⭐⭐ | 1-3배 |
| 데이터 크기(열) | ⭐⭐⭐⭐ | 0.5-2배 |

### 세부 설명

**데이터 크기**
- 행 2배 → 시간 2배 (선형)
- 열 2배 → 시간 1.5-2배

**하드웨어**
- RAM 부족 시 → 10배 이상 느려짐
- CPU 코어 수 → 병렬 처리 가능 알고리즘에 영향
- 저사양→중간: 2-3배, 중간→고사양: 2-3배

**데이터 타입**
- 수치형: 기준 (가장 빠름)
- 범주형: 1.5배 느림
- 텍스트: 2-3배 느림 (메모리 많이 사용)

**툴 선택**
- Pandas: 기준 (1x)
- Polars: 5-25배 빠름
- SQL: 집계 작업에 최적화

---



**문서 버전**: 1.2 (간소화)  
**작성일**: 2026년 1월  
**작성자**: Project Team
