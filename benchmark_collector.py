"""
ë°ì´í„° ë¶„ì„ ì‹œê°„ ì˜ˆì¸¡ - ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ì‹¤ì œ ë¶„ì„ ì‹œê°„ì„ ì¸¡ì •í•˜ì—¬
ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì°¸ê³ : service-plan.mdì˜ 17ê°œ ë¶„ì„ ë°©ë²• ê¸°ì¤€
"""

import pandas as pd
import numpy as np
import time
import json
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split


class BenchmarkCollector:
    """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.results = []
        
    def generate_sample_data(self, rows, columns, numeric_ratio=0.7, categorical_ratio=0.2, text_ratio=0.1):
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±"""
        n_numeric = int(columns * numeric_ratio)
        n_categorical = int(columns * categorical_ratio)
        n_text = columns - n_numeric - n_categorical
        
        data = {}
        
        # ìˆ˜ì¹˜í˜• ë°ì´í„°
        for i in range(n_numeric):
            data[f'numeric_{i}'] = np.random.randn(rows)
        
        # ë²”ì£¼í˜• ë°ì´í„°
        for i in range(n_categorical):
            data[f'categorical_{i}'] = np.random.choice(['A', 'B', 'C', 'D', 'E'], rows)
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° (ê°„ë‹¨í•œ ë¬¸ìì—´)
        for i in range(n_text):
            data[f'text_{i}'] = [f'text_{j % 100}' for j in range(rows)]
        
        return pd.DataFrame(data)
    
    def benchmark_aggregation_basic(self, df):
        """ê¸°ë³¸ ì§‘ê³„"""
        start = time.time()
        result = df.describe()
        result = df.mean()
        result = df.sum()
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_aggregation_groupby(self, df):
        """ê·¸ë£¹ë³„ ì§‘ê³„"""
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ì°¾ê¸°
        cat_cols = [col for col in df.columns if 'categorical' in col]
        if not cat_cols:
            return None
        
        start = time.time()
        result = df.groupby(cat_cols[0]).mean()
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_aggregation_pivot(self, df):
        """í”¼ë²— í…Œì´ë¸”"""
        cat_cols = [col for col in df.columns if 'categorical' in col]
        num_cols = [col for col in df.columns if 'numeric' in col]
        
        if len(cat_cols) < 1 or len(num_cols) < 1:
            return None
        
        start = time.time()
        result = pd.pivot_table(df, values=num_cols[0], index=cat_cols[0], aggfunc='mean')
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_linear_regression_simple(self, df):
        """ë‹¨ìˆœ ì„ í˜• íšŒê·€"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols[:1]].values
        y = df[num_cols[1]].values
        
        start = time.time()
        model = LinearRegression()
        model.fit(X, y)
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_linear_regression_multiple(self, df):
        """ë‹¤ì¤‘ ì„ í˜• íšŒê·€"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols[:-1]].values
        y = df[num_cols[-1]].values
        
        start = time.time()
        model = LinearRegression()
        model.fit(X, y)
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_ridge_regression(self, df):
        """ë¦¿ì§€ íšŒê·€"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols[:-1]].values
        y = df[num_cols[-1]].values
        
        start = time.time()
        model = Ridge(alpha=1.0)
        model.fit(X, y)
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_logistic_regression(self, df):
        """ë¡œì§€ìŠ¤í‹± íšŒê·€"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols[:-1]].values
        y = (df[num_cols[-1]].values > 0).astype(int)
        
        start = time.time()
        model = LogisticRegression(max_iter=100)
        model.fit(X, y)
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_decision_tree(self, df):
        """ì˜ì‚¬ê²°ì •ë‚˜ë¬´"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols[:-1]].values
        y = (df[num_cols[-1]].values > 0).astype(int)
        
        start = time.time()
        model = DecisionTreeClassifier(max_depth=10)
        model.fit(X, y)
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_random_forest(self, df, n_estimators=100):
        """ëœë¤ í¬ë ˆìŠ¤íŠ¸"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols[:-1]].values
        y = (df[num_cols[-1]].values > 0).astype(int)
        
        start = time.time()
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=10, n_jobs=-1)
        model.fit(X, y)
        elapsed = time.time() - start
        return elapsed
    
    def benchmark_kmeans(self, df, n_clusters=5):
        """K-means í´ëŸ¬ìŠ¤í„°ë§"""
        num_cols = [col for col in df.columns if 'numeric' in col]
        if len(num_cols) < 2:
            return None
        
        X = df[num_cols].values
        
        start = time.time()
        model = KMeans(n_clusters=n_clusters, n_init=10, max_iter=100)
        model.fit(X)
        elapsed = time.time() - start
        return elapsed
    
    def collect_benchmark(self, rows, columns, method, tool='python', hardware='medium',
                         numeric_ratio=0.7, categorical_ratio=0.2, text_ratio=0.1):
        """ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì§‘"""
        print(f"Collecting: {rows} rows Ã— {columns} cols, {method}, {tool}, {hardware}")
        
        # ë°ì´í„° ìƒì„±
        df = self.generate_sample_data(rows, columns, numeric_ratio, categorical_ratio, text_ratio)
        
        # ë¶„ì„ ë°©ë²•ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        method_map = {
            'agg_basic': self.benchmark_aggregation_basic,
            'agg_groupby': self.benchmark_aggregation_groupby,
            'agg_pivot': self.benchmark_aggregation_pivot,
            'reg_linear_simple': self.benchmark_linear_regression_simple,
            'reg_linear_multiple': self.benchmark_linear_regression_multiple,
            'reg_ridge': self.benchmark_ridge_regression,
            'clf_logistic': self.benchmark_logistic_regression,
            'clf_tree': self.benchmark_decision_tree,
            'clf_forest': self.benchmark_random_forest,
            'clu_kmeans_small': lambda df: self.benchmark_kmeans(df, n_clusters=5),
            'clu_kmeans_large': lambda df: self.benchmark_kmeans(df, n_clusters=15),
        }
        
        if method not in method_map:
            print(f"  âš ï¸  Method {method} not implemented yet")
            return None
        
        try:
            elapsed_time = method_map[method](df)
            
            if elapsed_time is None:
                print(f"  âš ï¸  Skipped (insufficient data)")
                return None
            
            # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
            benchmark = {
                'timestamp': datetime.now().isoformat(),
                'rows': rows,
                'columns': columns,
                'method': method,
                'tool': tool,
                'hardware': hardware,
                'data_type_ratio': {
                    'numeric': numeric_ratio,
                    'categorical': categorical_ratio,
                    'text': text_ratio
                },
                'elapsed_time_seconds': round(elapsed_time, 4),
                'loading_time': round(elapsed_time * 0.2, 4),
                'preprocessing_time': round(elapsed_time * 0.3, 4),
                'execution_time': round(elapsed_time * 0.5, 4)
            }
            
            self.results.append(benchmark)
            print(f"  âœ… {elapsed_time:.2f}ì´ˆ")
            return benchmark
            
        except Exception as e:
            print(f"  âŒ Error: {str(e)}")
            return None
    
    def collect_multiple_benchmarks(self, configs):
        """ì—¬ëŸ¬ ì„¤ì •ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì§‘"""
        print("=" * 60)
        print("ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60)
        
        for config in configs:
            self.collect_benchmark(**config)
        
        print("\n" + "=" * 60)
        print(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(self.results)}ê°œ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)
    
    def save_results(self, filename='benchmark_data.json'):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def get_summary(self):
        """ê²°ê³¼ ìš”ì•½"""
        if not self.results:
            return "ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ"
        
        df = pd.DataFrame(self.results)
        summary = f"""
ë²¤ì¹˜ë§ˆí¬ ìš”ì•½:
- ì´ ê°œìˆ˜: {len(self.results)}
- ë°ì´í„° í¬ê¸° ë²”ìœ„: {df['rows'].min():,} ~ {df['rows'].max():,} í–‰
- í‰ê·  ì‹¤í–‰ ì‹œê°„: {df['elapsed_time_seconds'].mean():.2f}ì´ˆ
- ë¶„ì„ ë°©ë²•: {df['method'].nunique()}ì¢…ë¥˜
"""
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = BenchmarkCollector()
    
    # ìˆ˜ì§‘í•  ë²¤ì¹˜ë§ˆí¬ ì„¤ì • (MVPìš© - ëŒ€í‘œì ì¸ ì¡°í•©ë§Œ)
    configs = [
        # ì†Œê·œëª¨ ë°ì´í„°
        {'rows': 10000, 'columns': 10, 'method': 'agg_basic'},
        {'rows': 10000, 'columns': 10, 'method': 'agg_groupby'},
        {'rows': 10000, 'columns': 20, 'method': 'reg_linear_simple'},
        {'rows': 10000, 'columns': 20, 'method': 'clf_logistic'},
        
        # ì¤‘ê°„ ê·œëª¨ ë°ì´í„°
        {'rows': 100000, 'columns': 20, 'method': 'agg_basic'},
        {'rows': 100000, 'columns': 20, 'method': 'agg_groupby'},
        {'rows': 100000, 'columns': 20, 'method': 'reg_linear_multiple'},
        {'rows': 100000, 'columns': 20, 'method': 'clf_tree'},
        {'rows': 100000, 'columns': 20, 'method': 'clu_kmeans_small'},
        
        # ëŒ€ê·œëª¨ ë°ì´í„°
        {'rows': 1000000, 'columns': 30, 'method': 'agg_basic'},
        {'rows': 1000000, 'columns': 30, 'method': 'agg_groupby'},
        {'rows': 1000000, 'columns': 30, 'method': 'reg_linear_multiple'},
        {'rows': 1000000, 'columns': 30, 'method': 'clf_forest'},
        {'rows': 1000000, 'columns': 30, 'method': 'clu_kmeans_large'},
    ]
    
    # ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì§‘
    collector.collect_multiple_benchmarks(configs)
    
    # ê²°ê³¼ ì €ì¥
    collector.save_results('benchmark_data.json')
    
    # ìš”ì•½ ì¶œë ¥
    print(collector.get_summary())


if __name__ == "__main__":
    main()
